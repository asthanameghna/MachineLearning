{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Open Information Extraction\n",
    "\n",
    "In this lab you will be implementing an *open information extraction* system, almost entirely from scratch. Information extraction takes a body of freeform text and extracts the contained information in a computer interpretable form. The word *open* simply means that the text/facts are arbitrary, so it will work with any input rather than a specific domain (e.g. legal texts).\n",
    "\n",
    "As an example, given the input:\n",
    "\n",
    "> \"Trolls really don't like the sun.\"\n",
    "  \n",
    "\n",
    "you may extract the \"fact\":\n",
    "```\n",
    "('Trolls', 'do not like', 'the sun')\n",
    "```\n",
    "\n",
    "The approach is based on the paper \"*Identifying Relations for Open Information Extraction*\", by Fader, Soderland & Etzioni. Parts have been updated with more recent, or more ML, techniques however (Q3 and Q4 match the paper; Q1 and Q2 don't). You don't have to read the paper as this workbook takes you through the process; this lab is also intended as a tutorial/opportunity to see one way such a system works. Some parts are rule based, as that's still often the case. Note that the several parts of a complete system have been dropped, as the lab would be too much work otherwise; consequentially it isn't going to work that well.\n",
    "\n",
    "The steps of the system are as follows:\n",
    "*  Tokenise and split on sentences *(provided)*\n",
    "\n",
    "\n",
    "1. Part of speech tagging - token level\n",
    "2. Part of speech tagging - sentence level\n",
    "3. Named entity resolution\n",
    "4. Relation extraction\n",
    "\n",
    "\n",
    "*  Summarise \"*20,000 leagues under the seas*\" by Jules Verne *(provided)*\n",
    "\n",
    "A simple NLP library, called `ogonek`, is provided. It has some basic functionality that you will require; this is for two reasons:\n",
    "1. This has to run on BUCS computers, and normal NLP libraries do not.\n",
    "2. To keep certain low level functionality consistent, so automarking is easier.\n",
    "\n",
    "Its documentation can be found below in a markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ogonek\n",
    "\n",
    "A tiny NLP library, that contains exactly the functionality I don't want you to implement for this coursework!\n",
    "\n",
    "\n",
    "\n",
    "### Tokenisation and sentence splitting\n",
    "`ogonek.Tokenise()`\n",
    "\n",
    "A class that tokenises some text and splits it into sentences. Construct an instance with `tokens = ogonek.Tokenise('My text')`; it then has the same interface as a list of lists:\n",
    "* `len(tokens)`: Number of extracted sentences (not words)\n",
    "* `tokens[i]`: Sentence i, where i ranges from 0 to one less than `len(tokens)`. A sentence is a list of tokens.\n",
    "\n",
    "\n",
    "\n",
    "### Word vectors\n",
    "`ogonek.Glove()`\n",
    "\n",
    "Constructing a `glove = ogonek.Glove()` object loads a heavily pruned Glove word vectors from the file `baby_glove.zip` into memory, and will then translate tokens into word vectors. Note that it automatically lowercases any token it is handed, so you don't need to. Has the following interface:\n",
    "* `glove.len_vec()` - Returns the length of the word vectors; should be 300.\n",
    "* `len(glove)` - Returns how many word vectors it knows of.\n",
    "* `token in glove` - Returns `True` if it has a word vector for that token, `False` otherwise.\n",
    "* `glove[token]` - Returns the word vector for the given token; raises an error if it does not have one.\n",
    "* `glove.decode(token)` - Returns the word vector for the given token, but if the word vector is unknown returns a vector of zeros instead (silent failure).\n",
    "* `glove.decodes(list of tokens)` - Returns a list of word vectors, one for each token. Has the same silent failure behaviour as `decode`.\n",
    "\n",
    "\n",
    "\n",
    "### Groningen Meaning Bank dataset\n",
    "`ogonek.GMB()`\n",
    "\n",
    "Provides access to the Groningen Meaning Bank dataset, which is supplied in the file `ner_dataset.csv`. Replicates the interface of the tokenisation system as far as it can. Construct with `gmb = ogonek.GMB()`; has the following interface:\n",
    "* `len(gmb)`: Number of sentences (not words) in data set\n",
    "* `gmb[i]`: Sentence i, where i ranges from 0 to one less than `len(gmb)`. A sentence is a list of tokens.\n",
    "* `gmb.pos(i)`: A list of POS tags that match with sentence i. Note that these are the full Penn Treebank tags (not the reduced set used below).\n",
    "* `gmb.ner(i)`: A list of named entities that match with sentence i. Using outside-inside scheme.\n",
    "\n",
    "\n",
    "\n",
    "### Pretty printing\n",
    "\n",
    "`ogonek.aligned_print(*)` takes multiple lists and prints them out, aligning them so that all elements in position 0 of all lists are aligned vertically (extra space added as required), and then elements in position 1 and so on. For showing tags and a sentence with everything aligned. Also does word wrap and colour coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ogonek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary giving descriptions of the reduced part of speech tags...\n",
    "rpos_desc = {'C' : 'Coordinating conjunction',\n",
    "             '0' : 'Cardinal number',\n",
    "             'D' : 'Determiner',\n",
    "             'E' : 'Existential there',\n",
    "             'I' : 'Preposition or subordinating conjunction',\n",
    "             'J' : 'Adjective',\n",
    "             'N' : 'Noun',\n",
    "             'P' : 'Predeterminer',\n",
    "             'S' : 'Possessive ending',\n",
    "             'M' : 'Pronoun',\n",
    "             'R' : 'Adverb',\n",
    "             'Z' : 'Particle',\n",
    "             'T' : 'to',\n",
    "             'V' : 'Verb',\n",
    "             'A' : 'Anything else',\n",
    "             '.' : 'All punctuation'}\n",
    "\n",
    "\n",
    "\n",
    "# Reduced list of part of speech tags as a list...\n",
    "num_to_rpos = ['C', '0', 'D', 'E', 'I', 'J', 'N', 'P',\n",
    "               'S', 'M', 'R', 'Z', 'T', 'V', 'A', '.']\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary that maps a reduced part of speech\n",
    "# tag to it's index in the above list; useful for vectors/matrices etc...\n",
    "rpos_to_num = {'C' : 0,\n",
    "               '0' : 1,\n",
    "               'D' : 2,\n",
    "               'E' : 3,\n",
    "               'I' : 4,\n",
    "               'J' : 5,\n",
    "               'N' : 6,\n",
    "               'P' : 7,\n",
    "               'S' : 8,\n",
    "               'M' : 9,\n",
    "               'R' : 10,\n",
    "               'Z' : 11,\n",
    "               'T' : 12,\n",
    "               'V' : 13,\n",
    "               'A' : 14,\n",
    "               '.' : 15}\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary that maps the full part of speech tags to the reduced set...\n",
    "pos_to_rpos = {'CC' : 'C',\n",
    "               'CD' : '0',\n",
    "               'DT' : 'D',\n",
    "               'EX' : 'E',\n",
    "               'FW' : 'A',\n",
    "               'IN' : 'I',\n",
    "               'JJ' : 'J',\n",
    "               'JJR' : 'J',\n",
    "               'JJS' : 'J',\n",
    "               'LS' : 'A',\n",
    "               'MD' : 'A',\n",
    "               'NN' : 'N',\n",
    "               'NNS' : 'N',\n",
    "               'NNP' : 'N',\n",
    "               'NNPS' : 'N',\n",
    "               'PDT' : 'P',\n",
    "               'POS' : 'S',\n",
    "               'PRP' : 'M',\n",
    "               'PRP$' : 'M',\n",
    "               'RB' : 'R',\n",
    "               'RBR' : 'R',\n",
    "               'RBS' : 'R',\n",
    "               'RP' : 'Z',\n",
    "               'SYM' : 'A',\n",
    "               'TO' : 'T',\n",
    "               'UH' : 'A',\n",
    "               'VB' : 'V',\n",
    "               'VBD' : 'V',\n",
    "               'VBG' : 'V',\n",
    "               'VBN' : 'V',\n",
    "               'VBP' : 'V',\n",
    "               'VBZ' : 'V',\n",
    "               'WDT' : 'D',\n",
    "               'WP' : 'M',\n",
    "               'WP$' : 'S',\n",
    "               'WRB' : 'R',\n",
    "               '-' : '.',\n",
    "               'LRB' : '.',\n",
    "               'RRB' : '.',\n",
    "               '``' : '.',\n",
    "               '\"' : '.',\n",
    "               '.' : '.',\n",
    "               ',' : '.',\n",
    "               ';' : '.',\n",
    "               ':' : '.',\n",
    "               '$' : '.'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load book, tokenise and split on sentences\n",
    "The below code reads in the book, chops it down to just the text of the book, and then tokenises it using the provided `ogonek` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop file, only keeping lines between indicators...\n",
    "lines = []\n",
    "record = False\n",
    "\n",
    "with open('20,000 Leagues Under the Seas.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        if record:\n",
    "            if line.startswith('***END OF THE PROJECT GUTENBERG'):\n",
    "                break\n",
    "      \n",
    "            lines.append(line)\n",
    "    \n",
    "        else:\n",
    "            if line.startswith('***START OF THE PROJECT GUTENBERG'):\n",
    "                record = True\n",
    "\n",
    "text = ''.join(lines)\n",
    "\n",
    "\n",
    "# Tokenise...\n",
    "under_the_seas = ogonek.Tokenise(text)\n",
    "\n",
    "\n",
    "# Print 10 random sentences to check it worked...\n",
    "numpy.random.seed(0)\n",
    "\n",
    "for i in range(10):\n",
    "    toks = numpy.random.choice(under_the_seas)\n",
    "    print('{:02d}. {}'.format(i+1, ' '.join(toks)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Part of speech tagging - token level\n",
    "\n",
    "The goal here is to train a classifier that indicates which of the part of speech tags (the reduced set provided above) each word is. For this initial approach you're going to treat words (tokens) individually, without context. For features the Glove word vectors are going to be used (provided by `ogonek.Glove()`).\n",
    "\n",
    "Instead of training a single classifier a slight modification of a random kitchen sink for each part of speech tag is going to be used. Specifically, a logistic random kitchen sink that indicates the probability that the word should be labelled with the associated tag. This is a *one vs all* classifier - you have a classifier for every tag, run them all on each word, and then select the tag with the highest probability (it's inconsistent - they won't sum to 1!). A logistic random kitchen sink is simply a normal kitchen sink that is pushed through a sigmoid function (in neural network terms, the final layer has a non-linearity),\n",
    "$$\\operatorname{Sig}(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "such that the final binary classifier is\n",
    "$$P(\\textrm{tag}) = \\operatorname{Sig}\\left(\\sum_{k \\in K} \\alpha_k \\phi\\left(\\vec{x} \\cdot \\vec{w}_k\\right)\\right)$$\n",
    "For the cost function you should maximise the log likelihood of the dataset. This will require gradient descent; Nestorov or better, including backtracking line search to select the initial step size, to get all marks (it will be very slow otherwise). You can either differentiate yourself, copy the equations from lecture 7 of ML1 (where they were derived), or use tensor flow; your choice! The non-linearity, $\\phi(\\cdot)$ is up to you ($\\sin$ works). Remember to include the original features plus the value `1` when creating the extended feature vector (so it has a bias term). It is suggested to use 300 random features, in addition to the 300 provided by glove (total of 601 - bias term is the +1), as that keeps the resulting data matrix during training small enough that it completes reasonably quickly.\n",
    "\n",
    "The Groningen Meaning Bank dataset has been provided; it can be accessed via the class `ogonek.GMB`. It includes lots of sentences, each as a list of tokens, plus part of speech tags as a list aligned with the sentence.\n",
    "Source: https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMB sentences = 47959\n",
      "\n",
      "\u001b[0mThe U.S.  space agency is  making final preparations to launch the first \n",
      "\u001b[31mDT  NNP   NN    NN     VBZ VBG    JJ    NNS          TO VB     DT  JJ    \n",
      "\u001b[34mO   B-geo O     O      O   O      O     O            O  O      O   O     \n",
      "\u001b[0m\n",
      "\u001b[0mdirect space probe to the distant planet of Pluto . \n",
      "\u001b[31mJJ     NN    NN    TO DT  JJ      NN     IN NNP   . \n",
      "\u001b[34mO      O     O     O  O   O       O      O  B-geo O \n",
      "\u001b[0m\n",
      "\u001b[0mOn Monday , the freighter Torgelow was hijacked off the eastern coast of \n",
      "\u001b[31mIN NNP    , DT  NN        NNP      VBD VBN      IN  DT  JJ      NN    IN \n",
      "\u001b[34mO  B-tim  O O   O         B-art    O   O        O   O   O       O     O  \n",
      "\u001b[0m\n",
      "\u001b[0mSomalia . \n",
      "\u001b[31mNNP     . \n",
      "\u001b[34mB-geo   O \n",
      "\u001b[0m\n",
      "\u001b[0mChile and Bolivia are associate members . \n",
      "\u001b[31mNNP   CC  NNP     VBP JJ        NNS     . \n",
      "\u001b[34mB-gpe O   B-gpe   O   O         O       O \n",
      "\u001b[0m\n",
      "\u001b[0mVenezuela has freed 11 Colombian soldiers who had been detained after entering \n",
      "\u001b[31mNNP       VBZ VBN   CD JJ        NNS      WP  VBD VBN  VBN      IN    VBG      \n",
      "\u001b[34mB-geo     O   O     O  B-gpe     O        O   O   O    O        O     O        \n",
      "\u001b[0m\n",
      "\u001b[0mVenezuelan territory without authorization . \n",
      "\u001b[31mJJ         NN        IN      NN            . \n",
      "\u001b[34mB-gpe      O         O       O             O \n",
      "\u001b[0m\n",
      "\u001b[0mHowever , the closing figure of 12,012 points was below the record level of \n",
      "\u001b[31mRB      , DT  NN      NN     IN CD     NNS    VBD IN    DT  NN     NN    IN \n",
      "\u001b[34mO       O O   O       O      O  O      O      O   O     O   O      O     O  \n",
      "\u001b[0m\n",
      "\u001b[0m12,049 points reached during trading Wednesday . \n",
      "\u001b[31mCD     NNS    VBN     IN     NN      NNP       . \n",
      "\u001b[34mO      O      O       O      O       B-tim     O \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load word vectors; in a seperate cell as this takes a couple seconds...\n",
    "glove = ogonek.Glove()\n",
    "\n",
    "\n",
    "# Groningen Meaning Bank dataset - a set of sentences each tagged\n",
    "# with part of speech and named entity recognitiuon tags...\n",
    "gmb = ogonek.GMB()\n",
    "print('GMB sentences = {}'.format(len(gmb)))\n",
    "print()\n",
    "\n",
    "\n",
    "# Print out 5 random sentences from GMB with POS and NER tags, to illustrate the data...\n",
    "numpy.random.seed(1)\n",
    "for _ in range(5):\n",
    "    i = numpy.random.randint(len(gmb))\n",
    "    ogonek.aligned_print(gmb[i], gmb.pos(i), gmb.ner(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"This function returns the sigmoid\n",
    "    value of a single number or array of\n",
    "    numbers.\n",
    "        \n",
    "    ARGUMENTS\n",
    "    ---------\n",
    "    x : number or array of numbers\n",
    "        \n",
    "    RETURNS\n",
    "    -------\n",
    "    sigmoid value\n",
    "    \"\"\"\n",
    "    return (1/(1+numpy.exp(-x)))  \n",
    "\n",
    "\n",
    "def alpha_gradient(y_train, alpha, ex):\n",
    "    \"\"\"This function calculates and \n",
    "    returns the gradient value of alpha\n",
    "    at every instance.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ---------\n",
    "    y_train : actual tags\n",
    "    alpha : current alpha value set\n",
    "    ex : the word extended feature set\n",
    "    \n",
    "    RETURNS\n",
    "    -------\n",
    "    grad : alpha gradient calculated as (y_train - ex @ alpha) @ ex\n",
    "    \"\"\"\n",
    "    grad = numpy.zeros((601,1))                    # initialize gradient as the same size as alpha\n",
    "    p = y_train\n",
    "    q = sigmoid(ex.dot(alpha))                     # y_predictions\n",
    "\n",
    "    grad += numpy.einsum('ij,ik->kj',(p-q), ex)    # enisum returns the transposed dot product\n",
    "    return grad\n",
    "    \n",
    "\n",
    "def log_likelihood(y_train, alpha, ex):\n",
    "    \"\"\"This function returns the calculated \n",
    "    log likelihood at every instance. \n",
    "    \n",
    "    ARGUMENTS\n",
    "    ---------\n",
    "    y_train : actual tags\n",
    "    alpha : current alpha value set\n",
    "    ex : the word extended feature set\n",
    "    \n",
    "    RETURNS\n",
    "    -------\n",
    "    ll : log likelihood calculated as Summation(y_train*log(y_pred) + (1-y_train)*log(1-y_pred))\n",
    "    \"\"\"\n",
    "    p = y_train\n",
    "    q = sigmoid(ex.dot(alpha))                        # y_predictions\n",
    "    q = numpy.clip(q, 1e-3, 1 - 1e-3)                 # clipping the prediction value so as to avoid Nan or inf value while taking log      \n",
    "    \n",
    "    ll = sum(p*numpy.log(q) + (1-p)*numpy.log(1-q))   # log likelihood expression\n",
    "    \n",
    "    return ll\n",
    "\n",
    "def backtracking(y_train, alpha, ex):\n",
    "    \"\"\"This function calculates and returns\n",
    "    the best step size for given set of \n",
    "    parameters. It is calculated using the \n",
    "    Armijo-Goldstein condition.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ---------\n",
    "    y_train : actual tags\n",
    "    alpha : current alpha value set\n",
    "    ex : the word extended feature set\n",
    "    \n",
    "    RETURNS\n",
    "    -------\n",
    "    ita : best step size \n",
    "    \"\"\"\n",
    "    ita = 1.0                  # initial step size\n",
    "    mu = 0.5                   # mu value from lecture\n",
    "\n",
    "    # Armijo-Goldstein condition: loss(alpha + ita*alpha_gradient) > loss(alpha) - ita*mu*2nd_order_matrix_norm(alpha_gradient)\n",
    "    while (log_likelihood(y_train, alpha + ita*alpha_gradient(y_train, alpha, ex), ex)) > (log_likelihood(y_train, alpha, ex) + ita*mu*numpy.linalg.norm(alpha_gradient(y_train, alpha, ex), ord=2)):\n",
    "        ita *= 0.8             # reduce step size by 0.8 factor at every iteration\n",
    "        if ita < 1e-6:         # if ita gets too low break and use the last updated ita value\n",
    "            break\n",
    "                                            \n",
    "    return ita\n",
    "\n",
    "def nestrov(y_train, initial_alpha, ex, ita):\n",
    "    \"\"\"This function returns the optimized alpha\n",
    "    values using Nestrov gradient descent. It is\n",
    "    obtained by maximizing the log likelihood.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ---------\n",
    "    y_train : actual tags\n",
    "    alpha : current alpha value set\n",
    "    ex : the word extended feature set\n",
    "    ita : best step size\n",
    "    \n",
    "    RETURNS\n",
    "    -------\n",
    "    alpha_new : best alpha value with maximum log likelihood\n",
    "    best_ll : best log likelihood\n",
    "    \"\"\"\n",
    "    iter_lim = 256\n",
    "    lamda = 0.8\n",
    "    phi = numpy.zeros((601,1))                              # initialize phi as same size as alpha\n",
    "    alpha = initial_alpha\n",
    "    iteration = 0                                           # iteration counter\n",
    "    alpha = alpha - lamda*phi                               # initial change in alpha \n",
    "    grad = alpha_gradient(y_train, alpha, ex)               # initial alpha gradient\n",
    "    best_ll = log_likelihood(y_train, alpha, ex)            # initial log likelihood\n",
    "    \n",
    "    for itr in range(iter_lim):                             # the loop runs for 256 iterations unless terminated earlier\n",
    "        \n",
    "        phi_new = lamda*phi + ita*grad                      # update phi gradient\n",
    "        alpha_new = alpha + phi_new                         # update alpha\n",
    "        \n",
    "        grad_new = alpha_gradient(y_train, alpha_new, ex)   # calculate new alpha gradient with updated alpha\n",
    "        new_ll = log_likelihood(y_train, alpha_new, ex)     # calculate new log likelihood with udated alpha\n",
    "        \n",
    "        if(itr%10==0):     \n",
    "            print('iteration: ',itr, 'loss:', new_ll)       # Prints log likelihood at every 10 iterations\n",
    "        \n",
    "        if new_ll > best_ll:                                # check whether new ll is better than best ll\n",
    "            best_ll = new_ll                                # update best ll\n",
    "            grad = grad_new                                 # update to corresponding gradient\n",
    "            alpha = alpha_new                               # update to corresponding alpha\n",
    "            phi = phi_new                                   # update to corresponding phi\n",
    "            iteration = 0                    \n",
    "        else:\n",
    "            iteration += 1                                  # if no improvement, count iterations with no improvements\n",
    "            if iteration > 8:                               # break out of for loop if no improvement uptil 8 iterations\n",
    "                break\n",
    "            \n",
    "    return alpha_new, best_ll\n",
    "\n",
    "def postag_to_y_value(tag, sentence_id):\n",
    "    \"\"\"This function returns the y_train \n",
    "    value for a given tag and sentence. If\n",
    "    tag is true returns 1 else returns 0.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ---------\n",
    "    tag: pos tag\n",
    "    sentence_id : gmb sentence number id\n",
    "    \n",
    "    RETURNS\n",
    "    -------\n",
    "    all_y : all y_train values for the sentence as a list\n",
    "    \"\"\"\n",
    "    pos_extended_tags = gmb.pos(sentence_id)        # get extended pos tags\n",
    "    pos_reduced_tags = []\n",
    "    all_y = []\n",
    "    for ex_tag in pos_extended_tags:\n",
    "        new_tag = pos_to_rpos[ex_tag]               # convert extended pos tags to rpos\n",
    "        \n",
    "        if new_tag==tag:\n",
    "            all_y.append(1)                         # for true tag, returns 1\n",
    "        else:\n",
    "            all_y.append(0)                         # for false tag, return 0\n",
    "    \n",
    "    return all_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 14387 sentences for training\n",
      "Training C\n",
      "iteration:  0 loss: [-70424.60384458]\n",
      "iteration:  10 loss: [-27195.22309082]\n",
      "Log Likelihood:  [-27166.3662656]\n",
      "  (took 26.2993 seconds)\n",
      "Training 0\n",
      "iteration:  0 loss: [-64077.57892017]\n",
      "iteration:  10 loss: [-41428.76240506]\n",
      "iteration:  20 loss: [-39009.3304513]\n",
      "Log Likelihood:  [-29986.45235023]\n",
      "  (took 28.7282 seconds)\n",
      "Training D\n",
      "iteration:  0 loss: [-100617.91248853]\n",
      "iteration:  10 loss: [-52948.05978461]\n",
      "Log Likelihood:  [-52843.46476118]\n",
      "  (took 24.726 seconds)\n",
      "Training E\n",
      "iteration:  0 loss: [-22886.67070605]\n",
      "iteration:  10 loss: [-20783.7123676]\n",
      "iteration:  20 loss: [-18596.3009858]\n",
      "iteration:  30 loss: [-16945.58659366]\n",
      "iteration:  40 loss: [-14936.62059014]\n",
      "iteration:  50 loss: [-13443.86192088]\n",
      "iteration:  60 loss: [-12243.38201377]\n",
      "iteration:  70 loss: [-11088.89421888]\n",
      "iteration:  80 loss: [-9943.36721975]\n",
      "iteration:  90 loss: [-9023.53886256]\n",
      "iteration:  100 loss: [-8236.1688178]\n",
      "iteration:  110 loss: [-7690.53519029]\n",
      "iteration:  120 loss: [-5970.75325042]\n",
      "iteration:  130 loss: [-5423.80268453]\n",
      "iteration:  140 loss: [-6136.51536511]\n",
      "Log Likelihood:  [-5059.06161965]\n",
      "  (took 86.0753 seconds)\n",
      "Training I\n",
      "iteration:  0 loss: [-268213.55744515]\n",
      "iteration:  10 loss: [-201015.70036608]\n",
      "Log Likelihood:  [-164122.41546364]\n",
      "  (took 23.0117 seconds)\n",
      "Training J\n",
      "iteration:  0 loss: [-185996.49685287]\n",
      "Log Likelihood:  [-185996.49685287]\n",
      "  (took 22.8441 seconds)\n",
      "Training N\n",
      "iteration:  0 loss: [-716895.4998399]\n",
      "iteration:  10 loss: [-1166191.34882939]\n",
      "Log Likelihood:  [-487391.97179149]\n",
      "  (took 23.8778 seconds)\n",
      "Training P\n",
      "iteration:  0 loss: [-21854.80645677]\n",
      "iteration:  10 loss: [-18637.74151448]\n",
      "iteration:  20 loss: [-15025.50876518]\n",
      "iteration:  30 loss: [-12387.28118475]\n",
      "iteration:  40 loss: [-10232.23149727]\n",
      "iteration:  50 loss: [-8595.32006629]\n",
      "iteration:  60 loss: [-7462.61228259]\n",
      "iteration:  70 loss: [-6088.35048153]\n",
      "iteration:  80 loss: [-5307.70479168]\n",
      "iteration:  90 loss: [-4691.93302935]\n",
      "iteration:  100 loss: [-4051.04344827]\n",
      "iteration:  110 loss: [-3760.6246763]\n",
      "iteration:  120 loss: [-3380.72020846]\n",
      "iteration:  130 loss: [-3097.54326253]\n",
      "iteration:  140 loss: [-2876.52710961]\n",
      "Log Likelihood:  [-2876.52710961]\n",
      "  (took 90.8655 seconds)\n",
      "Training S\n",
      "iteration:  0 loss: [-44531.1651799]\n",
      "iteration:  10 loss: [-43732.92973261]\n",
      "Log Likelihood:  [-43688.56006426]\n",
      "  (took 25.7345 seconds)\n",
      "Training M\n",
      "iteration:  0 loss: [-71498.01866656]\n",
      "iteration:  10 loss: [-69720.10433039]\n",
      "Log Likelihood:  [-69720.09985758]\n",
      "  (took 28.467 seconds)\n",
      "Training R\n",
      "iteration:  0 loss: [-71379.07734174]\n",
      "iteration:  10 loss: [-67695.61188396]\n",
      "iteration:  20 loss: [-57344.21487225]\n",
      "Log Likelihood:  [-56175.95373674]\n",
      "  (took 33.1551 seconds)\n",
      "Training Z\n",
      "iteration:  0 loss: [-27019.12489584]\n",
      "iteration:  10 loss: [-25551.40304803]\n",
      "iteration:  20 loss: [-22975.24404388]\n",
      "iteration:  30 loss: [-20801.3514343]\n",
      "iteration:  40 loss: [-19135.10659316]\n",
      "iteration:  50 loss: [-17477.40671163]\n",
      "iteration:  60 loss: [-15881.94635773]\n",
      "iteration:  70 loss: [-14597.28996891]\n",
      "iteration:  80 loss: [-13775.38615025]\n",
      "iteration:  90 loss: [-12898.22829336]\n",
      "iteration:  100 loss: [-10743.32080243]\n",
      "iteration:  110 loss: [-10184.23594014]\n",
      "Log Likelihood:  [-10184.23594014]\n",
      "  (took 79.588 seconds)\n",
      "Training T\n",
      "iteration:  0 loss: [-68334.91059916]\n",
      "iteration:  10 loss: [-18440.31412441]\n",
      "iteration:  20 loss: [-15357.92490915]\n",
      "iteration:  30 loss: [-12974.20259593]\n",
      "iteration:  40 loss: [-10694.09119126]\n",
      "iteration:  50 loss: [-9147.88603194]\n",
      "iteration:  60 loss: [-7904.64458842]\n",
      "iteration:  70 loss: [-7082.77430437]\n",
      "iteration:  80 loss: [-6474.94634925]\n",
      "iteration:  90 loss: [-6060.74560912]\n",
      "iteration:  100 loss: [-5514.90743503]\n",
      "iteration:  110 loss: [-52495.18935672]\n",
      "Log Likelihood:  [-4838.12340838]\n",
      "  (took 74.9312 seconds)\n",
      "Training V\n",
      "iteration:  0 loss: [-346243.58830785]\n",
      "iteration:  10 loss: [-346473.76383902]\n",
      "Log Likelihood:  [-346156.05792122]\n",
      "  (took 23.5901 seconds)\n",
      "Training A\n",
      "iteration:  0 loss: [-35670.9349087]\n",
      "iteration:  10 loss: [-33992.14101128]\n",
      "iteration:  20 loss: [-31214.21133147]\n",
      "iteration:  30 loss: [-21764.20046829]\n",
      "iteration:  40 loss: [-20530.19232387]\n",
      "Log Likelihood:  [-20529.30128549]\n",
      "  (took 40.2035 seconds)\n",
      "Training .\n",
      "iteration:  0 loss: [-199188.71891893]\n",
      "iteration:  10 loss: [-35277.81805458]\n",
      "Log Likelihood:  [-32762.25311456]\n",
      "  (took 23.9818 seconds)\n"
     ]
    }
   ],
   "source": [
    "# A test/train split - train with [0:split], test with [split:len(gmb)]\n",
    "split = int(len(gmb) * 0.3) # Have a lot of data, and don't want you waiting around too long to train!\n",
    "print('Using {} sentences for training'.format(split))\n",
    "x_temp = gmb[0:split]\n",
    "\n",
    "# Preparing the x_train list with all the sentence tokens together\n",
    "x_train = []\n",
    "for sntc in x_temp:           \n",
    "    for word in sntc:\n",
    "        x_train.append(word)\n",
    "        \n",
    "\n",
    "def train_tag_model(tag):\n",
    "    start = time.time()\n",
    "    \n",
    "\n",
    "    # Preparing y_train as a list of 0s and 1s for current tag \n",
    "    y_train = []\n",
    "    for i in range(len(x_temp)):\n",
    "        y_tag = postag_to_y_value(tag, i)\n",
    "        for j in y_tag:\n",
    "            y_train.append(j)\n",
    "    y_train = numpy.array(y_train)\n",
    "    \n",
    "    \n",
    "    #************* LOGISTIC RANDOM KITCHEN SINK IMPLEMENTATION ***************#\n",
    "    \n",
    "    x = numpy.array(glove.decodes(x_train))                         # exracting 300 features for all words  \n",
    "    w_k = numpy.random.normal(loc=0.0, scale=1.0, size=(300,300))   # random initialization of w_k\n",
    "    nf = numpy.sin(numpy.einsum('ef,gf->eg',x,w_k))                 # sin(x @ w_k)\n",
    "            \n",
    "    ex = numpy.append(x, nf, axis=1)                                # extending word features to 600\n",
    "    ones = numpy.ones((x.shape[0],1))                               \n",
    "            \n",
    "    ex = numpy.append(ex, ones, axis=1)                             # adding bias making it 601 features\n",
    "\n",
    "    alpha = numpy.random.normal(loc=0.0, scale=1.0, size=(601,1))   # random initialization of alpha\n",
    "#     ita = backtracking(y_train.reshape((313964, 1)), alpha, ex)\n",
    "    ita = 0.01      # Did not use backtracking as it did not improve the test accuracy but it has been fully implemented\n",
    "    updated_alpha, best_ll = nestrov(y_train.reshape((313964, 1)), alpha, ex, ita)  # finding best alpha\n",
    "    print('Log Likelihood: ', best_ll)\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    print('  (took {:g} seconds)'.format(end-start))\n",
    "    return updated_alpha, w_k                                       # return updated alpha and corresponding weights\n",
    "\n",
    "\n",
    "\n",
    "# Code to train a model for each reduced POS tag...\n",
    "rpos_model = {}\n",
    "for tag in rpos_desc:\n",
    "    print('Training {}'.format(tag))\n",
    "    rpos_model[tag] = train_tag_model(tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99%\n",
      "Percentage correct = 56.2%\n",
      "  (took 1141.07 seconds)\n"
     ]
    }
   ],
   "source": [
    "def predict(x, tag):\n",
    "    \"\"\"This function returns a probability\n",
    "    value for each tag for a given word.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ---------\n",
    "    x : array of 300 word features\n",
    "    tag : pos tag\n",
    "    \n",
    "    RETURNS\n",
    "    -------\n",
    "    y_pred : probability of word having the current tag\n",
    "    \"\"\"\n",
    "    \n",
    "    # Implementing logistic random kitchen sink for single word\n",
    "    \n",
    "    w_k = rpos_model[tag][1]\n",
    "           \n",
    "    nf = numpy.sin(numpy.einsum('f,gf->g',x,w_k))\n",
    "  \n",
    "    ex = numpy.append(x, nf, axis=0)\n",
    "    ones = numpy.ones((x.shape[0],1))\n",
    "\n",
    "    ex = numpy.append(ex, numpy.ones(1), axis=0)\n",
    "    \n",
    "    alpha = rpos_model[tag][0]\n",
    "    \n",
    "    temp = ex.dot(alpha)            \n",
    "    y_pred = sigmoid(temp)        # prediction proability is sigmoid(ex @ alpha)\n",
    "    \n",
    "    return y_pred\n",
    "    \n",
    "# You will want to train the models above, then fill in the below function to estimate POS tags...\n",
    "def token_pos(sentence):\n",
    "    \"\"\"Given a sentence, as a list of tokens, this should return part of\n",
    "    speech tags, as a list of strings (the codes in the rpos_desc dictionary).\n",
    "    Basically calls the models for each tag and selects the tag with the\n",
    "    highest probability.\"\"\"\n",
    "    \n",
    "    # **************************************************************** some of the above marks\n",
    "    # argmax on all probablities in the array og=f 16 probabilities\n",
    "    pred_tag = []\n",
    "    for token in sentence:  \n",
    "        x = glove.decode(token)                 # extracting 300 word features \n",
    "        \n",
    "        preds =[]\n",
    "        for tag in rpos_desc:\n",
    "            prediction = predict(x, tag)\n",
    "            preds.append(prediction)            # list containing probability of each tag for a token\n",
    "         \n",
    "        pred_ind = numpy.argmax(preds)          # pas with max probability is the token pos\n",
    "        pred_tag.append(num_to_rpos[pred_ind])\n",
    "        \n",
    "    return pred_tag                             # list of predicted tags for each token in sentence\n",
    "\n",
    "\n",
    "\n",
    "# Code to test the performance of your POS tagger...\n",
    "correct = 0\n",
    "tested = 0\n",
    "pershown = 0\n",
    "stop_percent = 100 # If you want faster feedback you can reduce this\n",
    "\n",
    "start = time.time()\n",
    "for i in range(split, len(gmb)):\n",
    "    percent = int(100 * (i - split) / (len(gmb) - split))\n",
    "    if percent>pershown:\n",
    "        pershown = percent\n",
    "        print('\\r{: 3d}%'.format(percent), end='')\n",
    "    \n",
    "    if percent>=stop_percent:\n",
    "        break\n",
    "    \n",
    "    guess = token_pos(gmb[i])\n",
    "    truth = gmb.pos(i)\n",
    "    \n",
    "    for g,t in zip(guess, truth):\n",
    "        if g==pos_to_rpos[t]:\n",
    "            correct += 1\n",
    "        tested += 1\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "print('Percentage correct = {:.1f}%'.format(100 * correct / tested))\n",
    "print('  (took {:g} seconds)'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Part of speech tagging - sentence level\n",
    "\n",
    "While the previous step works very well you need POS tags to be super accurate, as everything else depends on them. You will now introduce context. This is done by calculating transition probabilities between tags and solving a Markov random chain using the forward-backwards algorithm (or just forward if you keep links; it's dynamic programming) to find the maximum a posteriori (MAP) POS tag assignment for the entire sentence. The adjacency matrix should contain $\\log P(\\textrm{second pos tag} | \\textrm{first pos tag})$. lec 11 term 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.        , 0.66247107, 0.7991075 , 0.32963192, 0.66110712,\n",
      "       0.83313072, 0.92514595, 0.        , 0.        , 0.67531336,\n",
      "       0.64959269, 0.        , 0.46024548, 0.87616876, 0.51330437,\n",
      "       0.44990073]), array([0.6507813 , 0.75947945, 0.53535445, 0.        , 0.83328178,\n",
      "       0.86739916, 1.07018286, 0.        , 0.        , 0.23333881,\n",
      "       0.49281073, 0.        , 0.5944099 , 0.67554738, 0.28614207,\n",
      "       0.91936601]), array([0.22105121, 0.68637504, 0.43163794, 0.07368374, 0.52300953,\n",
      "       0.95200618, 1.05471547, 0.        , 0.07368374, 0.47715624,\n",
      "       0.5808226 , 0.17108834, 0.28054014, 0.76277725, 0.5182489 ,\n",
      "       0.53816987]), array([0.15957142, 0.        , 0.15957142, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.37051335, 0.        , 0.        , 1.2162616 , 0.5904844 ,\n",
      "       0.15957142]), array([0.34307544, 0.76715602, 0.9398202 , 0.31325425, 0.62479414,\n",
      "       0.83321273, 0.94709087, 0.27055008, 0.06924946, 0.75196319,\n",
      "       0.56506672, 0.06924946, 0.40392451, 0.7315617 , 0.06924946,\n",
      "       0.54329599]), array([0.68042178, 0.71831664, 0.39376139, 0.        , 0.79308114,\n",
      "       0.8539158 , 1.10320758, 0.07739838, 0.15479676, 0.38344678,\n",
      "       0.46439029, 0.        , 0.65185419, 0.54178867, 0.17971348,\n",
      "       0.72391843]), array([0.82447457, 0.6333624 , 0.69046624, 0.21537511, 0.98113803,\n",
      "       0.68428822, 1.        , 0.        , 0.7953191 , 0.61743207,\n",
      "       0.73015347, 0.15775942, 0.79458265, 0.96495453, 0.70520905,\n",
      "       0.97494668]), array([0.        , 0.        , 1.22767025, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.36084881,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        ]), array([0.17085792, 0.4576901 , 0.1354017 , 0.        , 0.08542896,\n",
      "       0.86096594, 0.94599521, 0.        , 0.        , 0.        ,\n",
      "       0.43461666, 0.        , 0.        , 0.43818931, 0.        ,\n",
      "       0.3491877 ]), array([0.47598911, 0.41258116, 0.55676962, 0.08884452, 0.6791147 ,\n",
      "       0.8309758 , 0.96740512, 0.        , 0.        , 0.58503781,\n",
      "       0.71075613, 0.44422258, 0.5632609 , 1.03671644, 0.77693053,\n",
      "       0.66561006]), array([0.48861648, 0.68695507, 0.81039589, 0.26691345, 0.86849643,\n",
      "       0.84467923, 0.79002673, 0.22998708, 0.08897115, 0.68199375,\n",
      "       0.75517288, 0.08897115, 0.70241101, 1.01515381, 0.40792938,\n",
      "       0.86321346]), array([0.274108  , 0.40372982, 0.78387727, 0.        , 0.79612241,\n",
      "       0.61580872, 0.74227615, 0.        , 0.        , 0.62117708,\n",
      "       0.4241578 , 0.        , 0.60150086, 0.31811835, 0.        ,\n",
      "       0.6102451 ]), array([0.08550814, 0.62761692, 0.83847091, 0.        , 0.39205161,\n",
      "       0.68502513, 0.83750227, 0.08550814, 0.        , 0.58318081,\n",
      "       0.54805509, 0.        , 0.08550814, 1.03838537, 0.        ,\n",
      "       0.44207067]), array([0.59030388, 0.72501478, 0.92230169, 0.44124987, 0.90920772,\n",
      "       0.82490082, 0.90431014, 0.30926687, 0.        , 0.79251786,\n",
      "       0.79138986, 0.66400633, 0.7993072 , 0.9199117 , 0.34882925,\n",
      "       0.7338266 ]), array([0.        , 0.15270303, 0.09634488, 0.        , 0.19268977,\n",
      "       0.09634488, 0.24904791, 0.        , 0.        , 0.09634488,\n",
      "       0.81303442, 0.        , 0.30540606, 1.03649976, 0.        ,\n",
      "       0.38537953]), array([0.72096333, 0.64008689, 0.73915222, 0.29059886, 0.70590194,\n",
      "       0.64900765, 0.79222443, 0.06968923, 0.2960348 , 0.60680916,\n",
      "       0.63939351, 0.        , 0.47792742, 0.77603901, 0.43136411,\n",
      "       0.59983823])]\n"
     ]
    }
   ],
   "source": [
    "# Adjacent Probabilities\n",
    "\n",
    "adj_prob = numpy.ones((16,16))        # initializing all as ones as we are going to work in log space\n",
    "x_temp = gmb[0:split]                 # calculating probabilities based on training set only\n",
    " \n",
    "    \n",
    "for i in range(len(x_temp)):          # for every sentence in training data\n",
    "    ext_tags = gmb.pos(i)             # get pos of tokens\n",
    "    tag_ind = []\n",
    "    for ex_tag in ext_tags:\n",
    "        ind = rpos_to_num[pos_to_rpos[ex_tag]]    # convert extended tags to rpos\n",
    "        tag_ind.append(ind)\n",
    "        \n",
    "    for j in range(len(tag_ind)-1):\n",
    "        adj_prob[tag_ind[j]][tag_ind[j+1]] += 1   # check the pair and update count accordingly in adj_prob matrix\n",
    "            \n",
    "log_adj_probs = numpy.log(adj_prob)               # taking log\n",
    "\n",
    "max_probs = numpy.amax(log_adj_probs, axis=0)     # finding max for normalization along first tag\n",
    "\n",
    "norm_adj_probs = []\n",
    "for i in range(16):\n",
    "    norm_prob = log_adj_probs[i]/max_probs[i]     # normalization\n",
    "    norm_adj_probs.append(norm_prob)  \n",
    "\n",
    "    \n",
    "print(norm_adj_probs)                 # normalized adjacent probabilities in log space\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.        , 0.37206308, 0.24254942, 0.68755156, 0.37335593,\n",
      "       0.21029981, 0.12308127, 1.        , 1.        , 0.35989026,\n",
      "       0.38427013, 1.        , 0.5637468 , 0.16950531, 0.51345384,\n",
      "       0.57355228]), array([0.59079836, 0.4635179 , 0.72595779, 1.35283281, 0.37709883,\n",
      "       0.33714897, 0.09969874, 1.35283281, 1.35283281, 1.07960398,\n",
      "       0.7757745 , 1.35283281, 0.65680663, 0.56179843, 1.01777383,\n",
      "       0.27629822]), array([1.17609248, 0.55274337, 0.89398991, 1.37350637, 0.77158826,\n",
      "       0.19690314, 0.05931348, 1.47221332, 1.37350637, 0.83301346,\n",
      "       0.69414171, 1.24302289, 1.09640095, 0.45039474, 0.77796563,\n",
      "       0.75127944]), array([0.46206713, 0.530472  , 0.46206713, 0.530472  , 0.530472  ,\n",
      "       0.530472  , 0.530472  , 0.530472  , 0.530472  , 0.530472  ,\n",
      "       0.37164083, 0.530472  , 0.530472  , 0.00908661, 0.27734393,\n",
      "       0.46206713]), array([0.95065217, 0.37917249, 0.14649484, 0.99083842, 0.57101556,\n",
      "       0.29015622, 0.13669708, 1.04838541, 1.31965276, 0.39964594,\n",
      "       0.65150263, 1.31965276, 0.86865358, 0.42713844, 1.31965276,\n",
      "       0.6808403 ]), array([0.58011356, 0.5313758 , 0.94879649, 1.45522537, 0.43521882,\n",
      "       0.35697746, 0.03635548, 1.35568088, 1.25613639, 0.96206243,\n",
      "       0.85795844, 1.45522537, 0.61685524, 0.75841395, 1.22409023,\n",
      "       0.52417116]), array([0.53971236, 0.87216556, 0.77282938, 1.59928417, 0.26718513,\n",
      "       0.78357648, 0.23437339, 1.9739445 , 0.59043037, 0.89987752,\n",
      "       0.70379062, 1.69951077, 0.59171148, 0.29533748, 0.74718321,\n",
      "       0.27795543]), array([0.32688968, 0.32688968, 0.00401854, 0.32688968, 0.32688968,\n",
      "       0.32688968, 0.32688968, 0.32688968, 0.32688968, 0.23198825,\n",
      "       0.32688968, 0.32688968, 0.32688968, 0.32688968, 0.32688968,\n",
      "       0.32688968]), array([0.62599734, 0.40967098, 0.65273811, 0.7548569 , 0.69042712,\n",
      "       0.10552382, 0.04139548, 0.7548569 , 0.7548569 , 0.7548569 ,\n",
      "       0.42707277, 0.7548569 , 0.7548569 , 0.42437831, 0.7548569 ,\n",
      "       0.49150255]), array([0.63662136, 0.6975379 , 0.55901488, 1.00855429, 0.44147698,\n",
      "       0.2955828 , 0.16451406, 1.09390793, 1.09390793, 0.53185741,\n",
      "       0.41107881, 0.66713973, 0.55277865, 0.09792612, 0.34750453,\n",
      "       0.454451  ]), array([0.85068391, 0.592036  , 0.43106021, 1.13980076, 0.35529288,\n",
      "       0.38635224, 0.45762307, 1.18795542, 1.37185043, 0.59850592,\n",
      "       0.50307502, 1.37185043, 0.57188033, 0.16404102, 0.95590574,\n",
      "       0.36218227]), array([0.47234971, 0.38901643, 0.14462137, 0.6485725 , 0.13674902,\n",
      "       0.25267187, 0.17136654, 0.6485725 , 0.6485725 , 0.24922058,\n",
      "       0.37588338, 0.6485725 , 0.26187033, 0.44405566, 0.6485725 ,\n",
      "       0.2562487 ]), array([1.        , 0.46131935, 0.25179873, 1.08496741, 0.69539501,\n",
      "       0.40427416, 0.25276124, 1.        , 1.08496741, 0.50547445,\n",
      "       0.54037805, 1.08496741, 1.        , 0.05314847, 1.08496741,\n",
      "       0.64569226]), array([0.93676515, 0.68564939, 0.31788516, 1.21461802, 0.34229375,\n",
      "       0.49945095, 0.35142336, 1.46064866, 2.0371557 , 0.55981632,\n",
      "       0.56191903, 0.79937578, 0.54716024, 0.32234036, 1.38690009,\n",
      "       0.6692232 ]), array([0.78109661, 0.6690166 , 0.710382  , 0.78109661, 0.63966739,\n",
      "       0.710382  , 0.59830199, 0.78109661, 0.78109661, 0.710382  ,\n",
      "       0.18435072, 0.78109661, 0.5569366 , 0.02033303, 0.78109661,\n",
      "       0.49823816]), array([0.56673187, 0.7188627 , 0.53251806, 1.37625942, 0.59506277,\n",
      "       0.70208251, 0.43268775, 1.7917966 , 1.36603427, 0.78145905,\n",
      "       0.72016697, 1.92288398, 1.02388919, 0.46313298, 1.11147607,\n",
      "       0.79457156])]\n"
     ]
    }
   ],
   "source": [
    "# Emission Probabilities\n",
    "# adj_prob: transition probability from tag 1 to tag 2\n",
    "# tag_prob: probability of tag 1\n",
    "\n",
    "# emm = adj_prob/tag_prob\n",
    "\n",
    "tag_prob = numpy.ones(16)\n",
    "\n",
    "# counting occurances of each tag\n",
    "\n",
    "for i in range(len(x_temp)):\n",
    "    ext_tags = gmb.pos(i)\n",
    "    tag_ind = []\n",
    "    for ex_tag in ext_tags:\n",
    "        ind = rpos_to_num[pos_to_rpos[ex_tag]]\n",
    "        tag_ind.append(ind)\n",
    "        \n",
    "    for j in range(len(tag_ind)):\n",
    "        tag_prob[tag_ind[j]] += 1\n",
    "               \n",
    "\n",
    "# calculating emission probabilities\n",
    "\n",
    "emm_probs = []\n",
    "for i in range(16):\n",
    "    emm_prob = adj_prob[i]/tag_prob[i]\n",
    "    emm_probs.append(emm_prob)\n",
    "\n",
    "# converting to log space     \n",
    "log_emm_probs = numpy.log(emm_probs)    \n",
    "\n",
    "# normalization\n",
    "\n",
    "max_emm_probs = -1*(numpy.amax((-1*log_emm_probs), axis=0))\n",
    "\n",
    "norm_emm_probs = []\n",
    "for i in range(16):\n",
    "    norm_prob = log_emm_probs[i]/max_emm_probs[i]\n",
    "    norm_emm_probs.append(norm_prob)\n",
    "    \n",
    "print(norm_emm_probs) # normalized log emission probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi\n",
    "\n",
    "def viterbi(words, tags, s_p, t_p, e_p):\n",
    "    \"\"\"This function implements the viterbi algorithm.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ---------\n",
    "    words : tokens in a given sentence\n",
    "    tags : list of all pos tags\n",
    "    s_p : start probabilities of each tag for each word\n",
    "    t_p : transition probabilities \n",
    "    e_p : emission probabilities\n",
    "    \n",
    "    RETURNS\n",
    "    -------\n",
    "    x : list of predicted tokens\n",
    "    \"\"\"\n",
    "    x = ['N']*len(words)         # Initialize the prediction list\n",
    "    \n",
    "    # As the algorithm encounter a number of errors which were not completely resolved\n",
    "    # in time, it has not been used. However, a complete implementation and the throught-\n",
    "    # process behind it has been explained in detail\n",
    "    \n",
    "    using_viterbi = 0\n",
    "    if using_viterbi:\n",
    "        x = ['N']*len(words)\n",
    "        T1 = [[0] * len(tags)]                  # probaility list\n",
    "        T2 = [[None] * len(tags)]               # tag list\n",
    "        \n",
    "        for j in range(len(tags)):              # Looping over each tag and initializing the first values with start probabilities\n",
    "            T1[0][j] = s_p[0][j] + e_p[j][0]    # T1[first] = start_prob * emission_prob\n",
    "        \n",
    "        # Looping over every word and tag\n",
    "        for i in range(1,len(words)-1):          \n",
    "            for j in range(len(tags)-1):\n",
    "                # For given word and tag, we find the T1[word][tag] which is maximum\n",
    "                \n",
    "                # Initializing with first tag values\n",
    "                # adding everything up instead of multiplying as we are working in log space\n",
    "                T1[i][j] = T1[i-1][j] + t_p[j][j] + e_p[j][i]  \n",
    "                T2[i][j] = T1[i-1][j] + t_p[j][j]\n",
    "            \n",
    "                # Looping over the rest of the tags to find the one with maximum T1[word][tag] value\n",
    "                for k in range(j, len(tags)-j):\n",
    "                    temp_T1 = T1[i-1][k] + t_p[k][j] + e_p[j][i]\n",
    "                    temp_T2 = T1[i-1][k] + t_p[k][j]\n",
    "                \n",
    "                    if temp_T1 > T1[i][j]:  # update if new calculated T1 is greater than best T1\n",
    "                        T1[i][j] = temp_T1\n",
    "                        T2[i][j] = numpy.where(T2 == temp_T2)  # update the corresponding tag in T2\n",
    "     \n",
    "    \n",
    "        z = [0]*len(words)     # stores the best T1 for each tag\n",
    "    \n",
    "        z[len(words)] = T1[0][len(words)]    # best T1 for each tag is the first element of the T1 list\n",
    "        x[len(words)] = tags[z[len(words)]]  # best probable tag index is the corresponding index\n",
    "    \n",
    "        # Looping over tags to find the best path \n",
    "        for k in range(1,len(tags)):\n",
    "            # For given word and tag, we find the T1[word][tag] which is maximum\n",
    "            \n",
    "            # Initialize the temps\n",
    "            temp_z = T1[k][len(words)]\n",
    "            temp_x = tags[z[len(words)]]\n",
    "        \n",
    "            # If new z is better than best z we update z and x\n",
    "            if temp_z > z[len(words)]:\n",
    "                z[len(words)] = temp_z\n",
    "                x[len(words)] = temp_x\n",
    "    \n",
    "        # Finally we complete the path for each word in reverse\n",
    "        for i in range(len(words), 2):\n",
    "            z[i-1] = T2[z[i]][i]\n",
    "            x[i-1] = tags[z[i-1]]\n",
    "        \n",
    "    return x    # return a list of predicted tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_pos(sentence):\n",
    "    \"\"\"Given a sentence, as a list of tokens, this should return part of\n",
    "    speech tags, as a list of strings (the codes in the rpos_desc dictionary).\n",
    "    A more advanced version of token_pos that uses neighbours as well.\"\"\"\n",
    "    \n",
    "    words = sentence\n",
    "    tags = num_to_rpos\n",
    "    trans = norm_adj_probs     # size(16,16)\n",
    "    emm = norm_emm_probs       # size(16,16)\n",
    "    \n",
    "    # we find start probabilities value from token_pos and take log\n",
    "    start_p = []\n",
    "    for token in sentence:  \n",
    "        x = glove.decode(token) \n",
    "        preds =[]\n",
    "        for tag in rpos_desc:\n",
    "            prediction = numpy.log(predict(x, tag)+1)  # adding 1 to avoid encountering log(0)\n",
    "            preds.append(prediction)\n",
    "        start_p.append(preds)\n",
    "        \n",
    "    all_tags = viterbi(words, tags, start_p, trans, emm)\n",
    "\n",
    "    return all_tags\n",
    "\n",
    "sentence = gmb[0]\n",
    "sentence_pos(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99%\n",
      "Percentage correct = 33.9%\n",
      "  (took 0.281279 seconds)\n"
     ]
    }
   ],
   "source": [
    "# Code to test the performance of your improved POS tagger...\n",
    "correct = 0\n",
    "tested = 0\n",
    "pershown = 0\n",
    "stop_percent = 100 # If you want faster feedback you can reduce this\n",
    "\n",
    "start = time.time()\n",
    "for i in range(split, len(gmb)):\n",
    "    percent = int(100 * (i - split) / (len(gmb) - split))\n",
    "    if percent>pershown:\n",
    "        pershown = percent\n",
    "        print('\\r{: 3d}%'.format(percent), end='')\n",
    "    \n",
    "    if percent>=stop_percent:\n",
    "        break\n",
    "    \n",
    "    guess = sentence_pos(gmb[i])\n",
    "    truth = gmb.pos(i)\n",
    "    \n",
    "    for g,t in zip(guess, truth):\n",
    "        if g==pos_to_rpos[t]:\n",
    "            correct += 1\n",
    "        tested += 1\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "print('Percentage correct = {:.1f}%'.format(100 * correct / tested))\n",
    "print('  (took {:g} seconds)'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named entity recognition\n",
    "\n",
    "The next step is to identify names, that is the entities that \"facts\" may apply to. While training a further classifier does work (same as above, inc. dynamic programming) there would be little point in repeating the exercise. Instead, a simple rule based approach using *regular expressions* is going to be used.\n",
    "\n",
    "You will probably want to look at the Python 3 documentation:\n",
    "https://docs.python.org/3/library/re.html\n",
    "There is also the how to, a tutorial:\n",
    "https://docs.python.org/3/howto/regex.html\n",
    "\n",
    "\n",
    "Given part of speech tagging a name can be defined as:\n",
    "* An optional *determiner*, e.g. *the* (1 or none)\n",
    "* An arbitrary number of *adjectives* (could be none)\n",
    "* A single *noun*\n",
    "\n",
    "Convert this into a regular expression and finish the function `sentence_ner()` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_ner(sentence, pos):\n",
    "    \"\"\"Given a sentence as a list of tokens and it's part of speech tags\n",
    "    this returns a list of the same length with True wherever it thinks\n",
    "    there is a name.\"\"\"\n",
    "    \n",
    "    pos_str = ''.join(pos)\n",
    "    \n",
    "    pattern = 'D*J*N+'    #RegEx pattern for optional determiner + arbitrary adjectives + single noun\n",
    "    tag_true = [p for p in re.finditer(pattern, pos_str)]   # find index where true\n",
    "    \n",
    "    ret = [False] * len(sentence)\n",
    "    \n",
    "    for i in range(len(sentence)):\n",
    "        if i in tag_true:\n",
    "            ret[i] = True\n",
    "    \n",
    "    # **************************************************************** 2 marks\n",
    "    \n",
    "    return ret     # list where presence of noun is expressed as True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99%\n",
      "Percentage correct = 84.7%\n",
      "  (took 0.619673 seconds)\n"
     ]
    }
   ],
   "source": [
    "# Code to test the performance of the NER tagger...\n",
    "correct = 0\n",
    "tested = 0\n",
    "pershown = 0\n",
    "stop_percent = 100 # If you want faster feedback you can reduce this\n",
    "\n",
    "start = time.time()\n",
    "for i in range(split, len(gmb)):\n",
    "    percent = int(100 * (i - split) / (len(gmb) - split))\n",
    "    if percent>pershown:\n",
    "        pershown = percent\n",
    "        print('\\r{: 3d}%'.format(percent), end='')\n",
    "    \n",
    "    if percent>=stop_percent:\n",
    "        break\n",
    "    \n",
    "    guess = sentence_ner(gmb[i], [pos_to_rpos[p] for p in gmb.pos(i)])\n",
    "    truth = [ner!='O' for ner in gmb.ner(i)]\n",
    "    \n",
    "    for g,t in zip(guess, truth):\n",
    "        if g==t:\n",
    "            correct += 1\n",
    "        tested += 1\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "print('Percentage correct = {:.1f}%'.format(100 * correct / tested))\n",
    "print('  (took {:g} seconds)'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Relation extraction\n",
    "\n",
    "This is where the paper \"*Identifying Relations for Open Information Extraction*\" comes in, specifically one of its novel contributions. It extracts relations using this procedure:\n",
    "1. Find relation text by matching a human-designed pattern to the POS tags\n",
    "2. Identify the named entities to the left and right of the relation text.\n",
    "3. Generate the relation tuple (left named entity, relation text, right named entity)\n",
    "\n",
    "(all previous approaches found names then relations - turns out it works much better the other way around)\n",
    "\n",
    "Relation text is identified as:\n",
    "`(Ve (Wo* Pa)?)+`\n",
    "where\n",
    "* `Ve = Verb Particle? Adverb?`\n",
    "* `Wo = Noun | Adjective | Adverb | Pronoun | Determiner`\n",
    "* `Pa = Preposition or subordinating conjunction | Particle`\n",
    "* `| =` or, so either of the options\n",
    "* `? =` optional\n",
    "* `+ =` at least one, but can be many\n",
    "* `* =` an arbitrary number of repetitions, including the option for none.\n",
    "\n",
    "You will need to convert the above rules into a regular expression - this one is harder than the above! You can then run it on a sentence, and for each match identify the named entity to the left and right and create a relation from that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(sentence):\n",
    "    \"\"\"Given a sentence, as a list of tokens, this returns a list of all relations\n",
    "    extracted from the sentence. Each relation is a tuple with three entries:\n",
    "    (named entity one, relation, named entity two)\"\"\"\n",
    "    pos = sentence_pos(sentence) \n",
    "    \n",
    "    str_pos = ''.join(pos)\n",
    "    \n",
    "    ner = sentence_ner(sentence, pos)\n",
    "    \n",
    "    relation_pattern = '((V(N*|J*|M*|R*|D*)I)?)+'       # pattern for relation\n",
    "    tag_true = [p for p in re.finditer(relation_pattern, str_pos)]  # find index for patter\n",
    "    rel_tag = sentence_pos(sentence)  #convert str_pos to sentence for tag_true.start() to tag_true.end()\n",
    "    \n",
    "    # Divide the string into two parts: front and rear of relation\n",
    "    for p in re.finditer(relation_pattern, str_pos):\n",
    "        for i in range(p.start(), p.end()):\n",
    "            if i == p.start():\n",
    "                tag_true_start = i\n",
    "            if i == p.end():\n",
    "                tag_true_end = i    \n",
    "            \n",
    "    front_str = str_pos[:tag_true[0]]   \n",
    "    rear_str = str_pos[tag_true[-1]:]\n",
    "    \n",
    "    front_sentence = sentence[0:front_str[tag_true_start]]  \n",
    "    rear_sentence = sentence[front_str[tag_true_end]:]\n",
    "    \n",
    "    front_sentence = ' '.join(front_sentence)\n",
    "    rear_sentence = ' '.join(rear_sentence)\n",
    "    \n",
    "    front_pos = sentence_pos(front_sentence)  # pos before tag_true.start()\n",
    "    rear_pos = sentence_pos(rear_sentence)    # pos after tag_true.end()\n",
    "    \n",
    "    front_noun = sentence_ner(front_sentence, front_pos)   # find noun in front \n",
    "    rear_noun = sentence_ner(rear_sentence, rear_pos)      # find noun in rear\n",
    "    \n",
    "    if front_noun and rear_noun:            # If either one of front and rear not found, discard relation\n",
    "        ret = [front_noun, rel_tag, rear_noun]\n",
    "    else:\n",
    "        ret = [None]\n",
    "    \n",
    "    \n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small test of the above...\n",
    "tests = ['London is full of pigeons.',\n",
    "         'In 1781 William Herschel discovered Uranus', # 1\n",
    "         \"Trolls really don't like the sun.\",\n",
    "         'Giant owls would enjoy eatting people.',\n",
    "         \"Dragons collect gold, but they don't make microprocessors.\"] # 2\n",
    "\n",
    "# 1. Seems to miss William - misclassified it, at least with the model answer.\n",
    "# 2. Should extract two facts, first sensible, second absurd.\n",
    "\n",
    "for sentence in tests:\n",
    "    print(sentence)\n",
    "    tokens = ogonek.Tokenise(sentence)\n",
    "    \n",
    "    rels = extract(tokens[0])\n",
    "    for rel in rels:\n",
    "        print('  ' + ' -- '.join(rel))\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
