{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Decision trees\n",
    "The following exercise takes you through for implementing decision trees. It involves data manipulation/visualisation, hyperparameter selection, recursion, and building a prediction model. We will use a binary classification problem: Breast cancer diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets as ds\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "The first step of any machine learning problem is to load the data. In this tutorial you don't have to download any dataset since we are using a built-in dataset provided by the scikit learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "data_all = ds.load_breast_cancer()\n",
    "\n",
    "x = data_all.data\n",
    "y = data_all.target\n",
    "\n",
    "y_names = data_all.target_names \n",
    "\n",
    "feature_names = data_all.feature_names\n",
    "print (y_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Wisconsin (Diagnostic) Database\n",
    "A description of the dataset used is provided here.\n",
    "\n",
    "Data Set Characteristics:\n",
    "    :Number of Instances: 569\n",
    "\n",
    "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
    "\n",
    "    :Attribute Information:\n",
    "        - radius (mean of distances from center to points on the perimeter)\n",
    "        - texture (standard deviation of gray-scale values)\n",
    "        - perimeter\n",
    "        - area\n",
    "        - smoothness (local variation in radius lengths)\n",
    "        - compactness (perimeter^2 / area - 1.0)\n",
    "        - concavity (severity of concave portions of the contour)\n",
    "        - concave points (number of concave portions of the contour)\n",
    "        - symmetry \n",
    "        - fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "        largest values) of these features were computed for each image,\n",
    "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
    "        13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "        - target class:\n",
    "                - WDBC-Malignant\n",
    "                - WDBC-Benign\n",
    "\n",
    "    :Summary Statistics:\n",
    "\n",
    "    ===================================== ====== ======\n",
    "                                           Min    Max\n",
    "    ===================================== ====== ======\n",
    "    radius (mean):                        6.981  28.11\n",
    "    texture (mean):                       9.71   39.28\n",
    "    perimeter (mean):                     43.79  188.5\n",
    "    area (mean):                          143.5  2501.0\n",
    "    smoothness (mean):                    0.053  0.163\n",
    "    compactness (mean):                   0.019  0.345\n",
    "    concavity (mean):                     0.0    0.427\n",
    "    concave points (mean):                0.0    0.201\n",
    "    symmetry (mean):                      0.106  0.304\n",
    "    fractal dimension (mean):             0.05   0.097\n",
    "    radius (standard error):              0.112  2.873\n",
    "    texture (standard error):             0.36   4.885\n",
    "    perimeter (standard error):           0.757  21.98\n",
    "    area (standard error):                6.802  542.2\n",
    "    smoothness (standard error):          0.002  0.031\n",
    "    compactness (standard error):         0.002  0.135\n",
    "    concavity (standard error):           0.0    0.396\n",
    "    concave points (standard error):      0.0    0.053\n",
    "    symmetry (standard error):            0.008  0.079\n",
    "    fractal dimension (standard error):   0.001  0.03\n",
    "    radius (worst):                       7.93   36.04\n",
    "    texture (worst):                      12.02  49.54\n",
    "    perimeter (worst):                    50.41  251.2\n",
    "    area (worst):                         185.2  4254.0\n",
    "    smoothness (worst):                   0.071  0.223\n",
    "    compactness (worst):                  0.027  1.058\n",
    "    concavity (worst):                    0.0    1.252\n",
    "    concave points (worst):               0.0    0.291\n",
    "    symmetry (worst):                     0.156  0.664\n",
    "    fractal dimension (worst):            0.055  0.208\n",
    "    ===================================== ====== ======\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
    "\n",
    "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
    "\n",
    "    :Donor: Nick Street\n",
    "\n",
    "    :Date: November, 1995\n",
    "\n",
    "This is a copy of the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset from https://goo.gl/U2Uwz2\n",
    "\n",
    "Features are computed from a digitized image of a fine needle\n",
    "aspirate (FNA) of a breast mass. They describe\n",
    "characteristics of the cell nuclei present in the image.\n",
    "\n",
    "Separating plane described above was obtained using\n",
    "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
    "Construction Via Linear Programming.\" Proceedings of the 4th\n",
    "Midwest Artificial Intelligence and Cognitive Science Society,\n",
    "pp. 97-101, 1992], a classification method which uses linear\n",
    "programming to construct a decision tree.  Relevant features\n",
    "were selected using an exhaustive search in the space of 1-4\n",
    "features and 1-3 separating planes.\n",
    "\n",
    "The actual linear program used to obtain the separating plane\n",
    "in the 3-dimensional space is that described in:\n",
    "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
    "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
    "Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "This database is also available through the UW CS ftp server:\n",
    "\n",
    "```\n",
    "ftp ftp.cs.wisc.edu\n",
    "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
    "```\n",
    "\n",
    "### References\n",
    "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
    "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
    "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
    "     San Jose, CA, 1993.\n",
    "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
    "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
    "     July-August 1995.\n",
    "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
    "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
    "     163-171.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare/Split data\n",
    "We provide the data preparation part. The bellow code block splits the data and the targets into training and test sets; 60% for training, 40% for test. This repartition is of course arbitrary, different percentages could be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 341\n",
      "Test set size: 228\n"
     ]
    }
   ],
   "source": [
    "split = int(x.shape[0] * 0.6)\n",
    "\n",
    "x_train = x[:split,:]\n",
    "y_train = y[:split]\n",
    "\n",
    "x_test = x[split:,:]\n",
    "y_test = y[split:]\n",
    "\n",
    "print('Training set size:', x_train.shape[0])\n",
    "print('Test set size:', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation\n",
    "\n",
    "Since our data has a feature dimensionality of 30, it is difficult for us to visualise it. We visualize data by using a dimensionality reduction technique called Principal Component Analysis (PCA). \n",
    "\n",
    "Given an array in `R^{nxd}` (a matrix of size `n X d` with real entries) with `n` and `d` being the number of data points and the feature dimensionality, respectively, PCA will output an array in `R^{nxm}`, with `m<d`. \n",
    "\n",
    "PCA will be covered in future lectures. But for now, you can consider it as a way to reduce the dimensionality of our feature space. \n",
    "\n",
    "In order to be able to visualise the data on a 2D plot, we choose `m=2` (`m=3` is also a possibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "x_scaled = preprocessing.scale(x[:,:-1]) # We remove the indexing and make sure all the features are in N(0,1)\n",
    "x_reduced = pca.fit_transform(x_scaled)\n",
    "\n",
    "                              ######## FIGURE WITHOUT SCALING #########\n",
    "# x_reduced = pca.fit_transform(x[:,0:-1]) # Uncomment this to see the result without scaling\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(x_reduced[:,0], x_reduced[:,1])\n",
    "# plt.title('Without Scaling')\n",
    "# plt.xlabel('Feature 1')\n",
    "# plt.ylabel('Feature 2')\n",
    "\n",
    "\n",
    "                                 ######## SCATTER PLOT ########\n",
    "    \n",
    "plt.figure()\n",
    "\n",
    "for target in range(len(y)):\n",
    "    if y[target] == 0:\n",
    "        # Malignant\n",
    "        plt.scatter(x_reduced[target,0], x_reduced[target,1], color = 'b')     # plotting each data point \n",
    "\n",
    "    if y[target] == 1:\n",
    "        # Benign\n",
    "        plt.scatter(x_reduced[target,0], x_reduced[target,1], color = 'r')\n",
    "        \n",
    "\n",
    "\n",
    "red_patch = mpatches.Patch(color='r', label='Benign')                         # red label for benign\n",
    "blue_patch = mpatches.Patch(color='b', label='Malignant')                     # blue label for malignant\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "plt.title('Dataset Visualisation using PCA')\n",
    "plt.xlabel('Reduced Feature 1')\n",
    "plt.ylabel('Reduced Feature 2')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Calculation\n",
    "Complete the function `calculate_entropy(y)` in the code block bellow. The input is a column vector of target class values, and the output is its entropy.\n",
    "\n",
    "`y` is an `n X 1` sized matrix where `n` is the number of data points (targets).\n",
    "The return is a scalar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    \n",
    "                   ######## ENTROPY CALCULATION ########\n",
    "        \n",
    "    #FORMULA USED: entropy = - summation(pi*log(pi))\n",
    "\n",
    "    sort = np.unique(y, return_counts=True)              # returns all the unique values in the array and the counts of their occurence\n",
    "    p_sum = 0                                            # summation variable\n",
    "    for i in range(len(sort[0])):\n",
    "        prob= (sort[1][i]).astype(float)/(float(569))    # calculates probability value of item in dataset\n",
    "        p_sum += prob*math.log(prob)                     # adds prob + log (prob) value to total sum\n",
    "\n",
    "    entropy = -1*p_sum                                   # calculates final entropy\n",
    "    return entropy\n",
    "       \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find split\n",
    "Use the function `calculate_entropy()` to complete the function `find_split(x, y)`.\n",
    "\n",
    "`find_split(x, y)` takes as input:\n",
    " * The data matrix of features, `x` in `R^{nXd}`. `n` is the number of data points and `d` is the feature dimensionality. \n",
    " * `y`, a column vector of size `n` containing the target value for each data point in `x`.\n",
    "\n",
    "`find_split(x, y)` outputs 'best_split' which is a dictionary (see the last part of the below code) with the following keys and their corresponding values:\n",
    "\n",
    " * `'feature'`: An integer indexing the attribute/feature chosen to split upon.\n",
    " * `'split'`: The value/threshold of this feature to split at.\n",
    " * `'infogain'`: A scalar representing the amount of information gained by splitting this way.\n",
    " * `'left_indices'`: Indices of the exemplars that satisfy `x[feature_index]<=split`.\n",
    " * `'right_indices'`: Opposite set of indices to `left_indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': 22, 'split': 105.0, 'infogain': 0.2813603408972909, 'left_indices': [3, 5, 9, 19, 20, 21, 37, 38, 40, 41, 46, 47, 48, 49, 50, 51, 52, 55, 58, 59, 60, 61, 63, 66, 67, 68, 69, 71, 74, 76, 79, 80, 81, 84, 88, 90, 92, 93, 96, 97, 98, 101, 102, 103, 104, 106, 107, 109, 110, 111, 113, 114, 115, 116, 120, 123, 124, 125, 130, 135, 136, 137, 139, 140, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 155, 158, 159, 160, 163, 165, 166, 169, 170, 173, 174, 175, 176, 178, 179, 183, 185, 187, 188, 189, 191, 192, 193, 195, 200, 204, 206, 208, 211, 215, 216, 217, 220, 221, 222, 224, 226, 228, 231, 232, 234, 235, 238, 240, 241, 242, 243, 245, 246, 247, 248, 249, 251, 266, 267, 268, 269, 270, 271, 273, 275, 276, 278, 279, 281, 284, 285, 286, 287, 288, 289, 290, 292, 293, 294, 295, 296, 297, 299, 301, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 318, 319, 320, 322, 324, 325, 326, 327, 331, 332, 333, 334, 336, 338], 'right_indices': [0, 1, 2, 4, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 39, 42, 43, 44, 45, 53, 54, 56, 57, 62, 64, 65, 70, 72, 73, 75, 77, 78, 82, 83, 85, 86, 87, 89, 91, 94, 95, 99, 100, 105, 108, 112, 117, 118, 119, 121, 122, 126, 127, 128, 129, 131, 132, 133, 134, 138, 141, 147, 148, 156, 157, 161, 162, 164, 167, 168, 171, 172, 177, 180, 181, 182, 184, 186, 190, 194, 196, 197, 198, 199, 201, 202, 203, 205, 207, 209, 210, 212, 213, 214, 218, 219, 223, 225, 227, 229, 230, 233, 236, 237, 239, 244, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 272, 274, 277, 280, 282, 283, 291, 298, 300, 302, 317, 321, 323, 328, 329, 330, 335, 337, 339, 340]}\n"
     ]
    }
   ],
   "source": [
    "def find_split(x, y):\n",
    "    \"\"\"Given a dataset and its target values, this finds the optimal combination\n",
    "    of feature and split point that gives the maximum information gain.\"\"\"\n",
    "    \n",
    "    # Need the starting entropy so we can measure improvement...\n",
    "    start_entropy = calculate_entropy(y)\n",
    "    \n",
    "    # Best thus far, initialised to a dud that will be replaced immediately...\n",
    "    best = {'infogain' : -np.inf}\n",
    "    \n",
    "    # Loop every possible split of every dimension...\n",
    "    for i in range(x.shape[1]):\n",
    "        for split in np.unique(x[:,i]):\n",
    "            \n",
    "            ######## INFOGAIN CALCULATION FOR EACH SPLIT FOR EVERY DIMENSION PAIR ######## \n",
    "            \n",
    "            # FORMULA USED: InfoGain = H(parent) - (nl/n)*H(p_left) - (nr/n)*H(p_right)\n",
    "            \n",
    "            left_indices = []\n",
    "            right_indices = []\n",
    "            \n",
    "            for index in range(x.shape[0]):\n",
    "                if x[index, i]>split:                      # checks whether element is right of split value\n",
    "                    right_indices.append(index)            # appends index to right_indices\n",
    "                else:\n",
    "                    left_indices.append(index)             # appends index to left_indices\n",
    "            \n",
    "            nl = float(len(left_indices))                  # converts denominator to float to avoid integer division\n",
    "            nr = float(len(right_indices))\n",
    "            n = nl + nr                                    # total indices\n",
    "            \n",
    "            infogain = start_entropy - (nl/n)*calculate_entropy(y[left_indices]) - (nr/n)*calculate_entropy(y[right_indices])  # infogain formula used\n",
    "            if infogain > best['infogain']:\n",
    "                best = {'feature' : i,\n",
    "                        'split' : split,\n",
    "                        'infogain' : infogain, \n",
    "                        'left_indices' : left_indices,\n",
    "                        'right_indices' : right_indices}            \n",
    "    return best\n",
    "\n",
    "\n",
    "# SAMPLE PRINT TO CHECK FUNCTION\n",
    "best = find_split(x_train, y_train)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The function `find_split()` allows us to find the optimal feature and the best value to split the data into two chunks. Applying this to the original data set splits it into two new data sets. We can then repeat this on both of the new data sets to get four data sets, and so on. This recursion builds a decision tree. It needs a stopping condition, to prevent it dividing the data forever, here we will use two:\n",
    " * Maximum depth: The tree is limited to be no deeper than a provided limit.\n",
    " * Perfection: If a node contains only one class then there is little point in splitting it further.\n",
    "\n",
    "We provide the function `build_tree(x, y, max_depth)` below to construct a tree. The inputs are: \n",
    "\n",
    " * The data matrix of features, `x` in `R^{nXd}`. `n` is the number of data points and `d` is the feature dimensionality. \n",
    " * `y`, a column vector of size `n` containing the target value for each data point in `x`.\n",
    " * The maximum depth of the tree, `max_depth`.\n",
    "\n",
    "The output of this function is a dictionary. If it has generated a leaf node then the keys are:\n",
    " * `'leaf' : True`\n",
    " * `'class'` : The index of the class to assign to exemplars that land here.\n",
    "\n",
    "If it has generated a split node then the keys are:\n",
    " * `'leaf' : False`\n",
    " * `'feature'`: The feature to apply the `split` to.\n",
    " * `'split'`: The split to test the exemplars `feature` with.\n",
    " * `'infogain'`: The information gain of this split.\n",
    " * `'left'` : The left subtree, for exemplars where `x[feature_index]<=split`\n",
    " * `'right'` : The right subtree, for exemplars where `x[feature_index]>split`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'leaf': False, 'feature': 22, 'split': 105.0, 'infogain': 0.2813603408972909, 'left': {'leaf': False, 'feature': 21, 'split': 23.31, 'infogain': 0.1230823726015435, 'left': {'leaf': True, 'class': 1}, 'right': {'leaf': False, 'feature': 7, 'split': 0.02645, 'infogain': 0.12430612231752462, 'left': {'leaf': True, 'class': 1}, 'right': {'leaf': True, 'class': 1}}}, 'right': {'leaf': False, 'feature': 1, 'split': 19.65, 'infogain': 0.1360323932794543, 'left': {'leaf': False, 'feature': 20, 'split': 18.22, 'infogain': 0.11961118715898023, 'left': {'leaf': True, 'class': 1}, 'right': {'leaf': True, 'class': 0}}, 'right': {'leaf': True, 'class': 0}}}\n"
     ]
    }
   ],
   "source": [
    "def build_tree(x, y, max_depth = np.inf):\n",
    "    # Check if either of the stopping conditions have been reached. If so generate a leaf node...\n",
    "    if max_depth==1 or (y==y[0]).all():\n",
    "        # Generate a leaf node...\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        return {'leaf' : True, 'class' : classes[np.argmax(counts)]}\n",
    "    \n",
    "    else:\n",
    "        move = find_split(x, y)\n",
    "        \n",
    "        left = build_tree(x[move['left_indices'],:], y[move['left_indices']], max_depth - 1)\n",
    "        right = build_tree(x[move['right_indices'],:], y[move['right_indices']], max_depth - 1)\n",
    "        \n",
    "        return {'leaf' : False,\n",
    "                'feature' : move['feature'],\n",
    "                'split' : move['split'],\n",
    "                'infogain' : move['infogain'],\n",
    "                'left' : left,\n",
    "                'right' : right}\n",
    "\n",
    "# SAMPLE PRINT TO CHECK FUNCTION\n",
    "tree = build_tree(x_train, y_train, 4)    \n",
    "print (tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After building the tree we should be able to predict the class of a sample. We do that by propagating the sample through the tree, i.e. we check all the splitting conditions until the sample falls in a leaf node, in which case the class of the leaf node is attributed to the sample.\n",
    "\n",
    "We provide the recursive function `predict_one(tree, sample)` that takes as input the constructed tree, a sample in `R^d` and recursively propagates it through the branches of our tree. The output of this function is the class predicted for the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(tree, sample):\n",
    "    \"\"\"Does the prediction for a single data point\"\"\"\n",
    "    if tree['leaf']:\n",
    "        return tree['class']\n",
    "    \n",
    "    else:\n",
    "        if sample[tree['feature']] <= tree['split']:\n",
    "            return predict_one(tree['left'], sample)\n",
    "        else:\n",
    "            return predict_one(tree['right'], sample)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further generalize the prediction function above to the case where we have a data matrix `R^{nXd}` representing many data points. the function `predict(tree, samples)` bellow takes as input the constructed tree and a data array then returns an array containing the predictions for all the samples in our input data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0\n",
      " 0 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "def predict(tree, samples):\n",
    "    \"\"\"Predicts class for every entry of a data matrix.\"\"\"\n",
    "    ret = np.empty(samples.shape[0], dtype=int)\n",
    "    ret.fill(-1)\n",
    "    indices = np.arange(samples.shape[0])\n",
    "    \n",
    "    def tranverse(node, indices):\n",
    "        nonlocal samples\n",
    "        nonlocal ret\n",
    "        \n",
    "        if node['leaf']:\n",
    "            ret[indices] = node['class']\n",
    "        \n",
    "        else:\n",
    "            going_left = samples[indices, node['feature']] <= node['split']\n",
    "            left_indices = indices[going_left]\n",
    "            right_indices = indices[np.logical_not(going_left)]\n",
    "            \n",
    "            if left_indices.shape[0] > 0:\n",
    "                tranverse(node['left'], left_indices)\n",
    "                \n",
    "            if right_indices.shape[0] > 0:\n",
    "                tranverse(node['right'], right_indices)\n",
    "    \n",
    "    tranverse(tree, indices)\n",
    "    return ret\n",
    "\n",
    "# SAMPLE PRINT TO CHECK FUNCTION\n",
    "pred1 = predict(tree, x_train) \n",
    "print (pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Testing\n",
    "Using the functions defined above build a tree and report the training and test accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Max Depth:  3\n",
      "Correct Predictions for training data:  316\n",
      "Correct Predictions for test data:  205\n",
      "Accuracy of Training Data:  92.66862170087977 %\n",
      "Accuracy of Test Data:  89.91228070175438 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decision_tree_changable_depth(max_depth):\n",
    "    \"\"\"Takes different maximum depth values and builds decision trees accordingly.\"\"\"\n",
    "    \n",
    "    tree = build_tree(x_train, y_train, max_depth)                # builds tree\n",
    "    pred_train = predict(tree, x_train)                           # predicts for training data\n",
    "    pred_test = predict(tree, x_test)                             # predicts for test data\n",
    "    \n",
    "#     print(pred_train)\n",
    "#     print(y_train)\n",
    "#     print(pred_test)\n",
    "#     print(y_test)\n",
    "\n",
    "    predicted_train_correct = 0                                   # counts the number of correct predictions\n",
    "    \n",
    "    for i in range(len(pred_train)):                              # loops over all predictions\n",
    "        if pred_train[i]==y_train[i]:                             # checks whether prediction is same as actual\n",
    "            predicted_train_correct += 1                          # increments the counter by 1\n",
    "            \n",
    "    predicted_test_correct = 0\n",
    "    \n",
    "    for i in range(len(pred_test)):\n",
    "        if pred_test[i]==y_test[i]:\n",
    "            predicted_test_correct += 1        \n",
    "   \n",
    "    print ('For Max Depth: ', max_depth)\n",
    "    print('Correct Predictions for training data: ', predicted_train_correct)\n",
    "    print('Correct Predictions for test data: ', predicted_test_correct)\n",
    "    \n",
    "    acc_train = 100 * predicted_train_correct/float(len(y_train))  # calculates accuracy as correct predictions percentage of total entries \n",
    "    acc_test = 100 * predicted_test_correct/float(len(y_test))\n",
    "    \n",
    "    \n",
    "    print ('Accuracy of Training Data: ', acc_train, '%') \n",
    "    print ('Accuracy of Test Data: ', acc_test, '%')\n",
    "    print ()\n",
    "    \n",
    "    return acc_train, acc_test\n",
    "\n",
    "# SAMPLE PRINT TO CHECK FUNCTION\n",
    "prediction = decision_tree_changable_depth(3)\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best parameter\n",
    "\n",
    "Find the best `max_depth` parameter plus its corresponding training and test accuracies. A good range to test is `range(2,6)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Max Depth:  2\n",
      "Correct Predictions for training data:  316\n",
      "Correct Predictions for test data:  205\n",
      "Accuracy of Training Data:  92.66862170087977 %\n",
      "Accuracy of Test Data:  89.91228070175438 %\n",
      "\n",
      "For Max Depth:  3\n",
      "Correct Predictions for training data:  316\n",
      "Correct Predictions for test data:  205\n",
      "Accuracy of Training Data:  92.66862170087977 %\n",
      "Accuracy of Test Data:  89.91228070175438 %\n",
      "\n",
      "For Max Depth:  4\n",
      "Correct Predictions for training data:  320\n",
      "Correct Predictions for test data:  214\n",
      "Accuracy of Training Data:  93.841642228739 %\n",
      "Accuracy of Test Data:  93.85964912280701 %\n",
      "\n",
      "For Max Depth:  5\n",
      "Correct Predictions for training data:  329\n",
      "Correct Predictions for test data:  205\n",
      "Accuracy of Training Data:  96.48093841642229 %\n",
      "Accuracy of Test Data:  89.91228070175438 %\n",
      "\n",
      "Best Max Depth is:  4\n",
      "Its training data accuracy is:  93.841642228739 %\n",
      "Its test data accuracy is:  93.85964912280701 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "       ######## CALCULATES BEST MAXIMUM DEPTH PARAMETER FOR THE DECISION TREE ########\n",
    "    \n",
    "best_max_depth = 0\n",
    "best_acc_train = 0\n",
    "best_acc_test = 0\n",
    "for max_depth in range(2,6):\n",
    "    acc_train, acc_test = decision_tree_changable_depth(max_depth)     # calculates test and training data accuracies\n",
    "    if acc_test > best_acc_test:                                       # checks whther test accuracy is better than the best test accuracy\n",
    "        best_max_depth = max_depth                                     # if yes, assigns new values best variables \n",
    "        best_acc_train = acc_train\n",
    "        best_acc_test = acc_test\n",
    "        \n",
    "print ('Best Max Depth is: ',  best_max_depth)\n",
    "print ('Its training data accuracy is: ', best_acc_train, '%')\n",
    "print ('Its test data accuracy is: ', best_acc_test, '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Recursive Tree\n",
    "Write a recursive function that prints out a tree, and use it to print the best tree learned.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "    [x22 <=0.296]\n",
    "\n",
    "        [x27 <=-0.058]\n",
    "\n",
    "          [x13 <=0.187]\n",
    "\n",
    "           [x21 <=1.246]\n",
    "                predict 1\n",
    "                \n",
    "           [x21 >1.246]\n",
    "                predict 0\n",
    "                \n",
    "          [x13 >0.187]\n",
    "\n",
    "           [x0 <=0.160]\n",
    "                predict 1\n",
    "                \n",
    "           [x0 >0.160]\n",
    "                predict 0\n",
    "                \n",
    "        [x27 >-0.058]\n",
    "\n",
    "          [x27 <=0.690]\n",
    "\n",
    "           [x21 <=0.263]\n",
    "            predict 1\n",
    "            \n",
    "           [x21 >0.263]\n",
    "            predict 0\n",
    "            \n",
    "          [x27 >0.690]\n",
    "            predict 0\n",
    "            \n",
    "    [x22 >0.296]\n",
    "\n",
    "    predict 0\n",
    "```\n",
    "\n",
    "The conditions with the same tree depth must be indented the same amount. This function should have as input the tree learned, and a scalar `indent` that is used to measure how for to indent at the current recursion level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [x 22  <= 105.0 ]\n",
      "\n",
      "     [x 21  <= 23.31 ]\n",
      "         predict  1\n",
      "\n",
      "     [x 21  > 23.31 ]\n",
      "\n",
      "         [x 7  <= 0.02645 ]\n",
      "             predict  1\n",
      "\n",
      "         [x 7  > 0.02645 ]\n",
      "             predict  1\n",
      "\n",
      " [x 22  > 105.0 ]\n",
      "\n",
      "     [x 1  <= 19.65 ]\n",
      "\n",
      "         [x 20  <= 18.22 ]\n",
      "             predict  1\n",
      "\n",
      "         [x 20  > 18.22 ]\n",
      "             predict  0\n",
      "\n",
      "     [x 1  > 19.65 ]\n",
      "         predict  0\n"
     ]
    }
   ],
   "source": [
    "def print_tree(tree, indent = 0):\n",
    "    # **************************************************************** 1 mark\n",
    "    if tree['leaf'] == False:\n",
    "        print()\n",
    "        print(' '*indent, '[x', tree['feature'], ' <=', tree['split'], ']')\n",
    "        print_tree(tree['left'], indent+4)\n",
    "        \n",
    "        print()\n",
    "        print(' '*indent, '[x', tree['feature'], ' >', tree['split'], ']')\n",
    "        print_tree(tree['right'], indent+4)\n",
    "    \n",
    "    else:\n",
    "        print(' '*indent,'predict ', tree['class'])\n",
    "    \n",
    "    pass\n",
    "\n",
    "print_tree(tree, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
